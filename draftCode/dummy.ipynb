{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f06a947-0782-4e71-bb81-b0068e5ebe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.4711 - val_loss: 1.1592\n",
      "Epoch 2/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8151 - val_loss: 0.4435\n",
      "Epoch 3/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3163 - val_loss: 0.2760\n",
      "Epoch 4/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1968 - val_loss: 0.2356\n",
      "Epoch 5/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1664 - val_loss: 0.2262\n",
      "Epoch 6/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1601 - val_loss: 0.2238\n",
      "Epoch 7/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1598 - val_loss: 0.2232\n",
      "Epoch 8/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1581 - val_loss: 0.2228\n",
      "Epoch 9/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1580 - val_loss: 0.2228\n",
      "Epoch 10/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1577 - val_loss: 0.2228\n",
      "Epoch 11/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1586 - val_loss: 0.2234\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_5\" is incompatible with the layer: expected shape=(None, 409), found shape=(32, 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 161\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Contoh penggunaan\u001b[39;00m\n\u001b[0;32m    160\u001b[0m user_preferences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMachine Learning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Science\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Preferensi pengguna\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m recommendations, combined_features, clusters \u001b[38;5;241m=\u001b[39m \u001b[43mcluster_and_recommend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_preferences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTop 3 Recommended Learning Paths:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(recommendations[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCourse Name_x\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[10], line 115\u001b[0m, in \u001b[0;36mcluster_and_recommend\u001b[1;34m(data, user_preferences)\u001b[0m\n\u001b[0;32m    112\u001b[0m autoencoder, encoder \u001b[38;5;241m=\u001b[39m train_and_get_encoder(X_scaled)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Gunakan encoder untuk mendapatkan fitur numerik\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m numeric_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumeric_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Ekstraksi fitur teks dengan TF-IDF\u001b[39;00m\n\u001b[0;32m    118\u001b[0m tfidf_vectors \u001b[38;5;241m=\u001b[39m vectorize_learning_paths(data)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"functional_5\" is incompatible with the layer: expected shape=(None, 409), found shape=(32, 7)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "data = pd.read_csv(\"data/updatemergedata.csv\")\n",
    "data.head()\n",
    "label_encoder = LabelEncoder()\n",
    "data['Level'] = label_encoder.fit_transform(data['Level']) \n",
    "data['Price'] = data['Price'].map({'Berbayar': 1, 'Gratis': 0})\n",
    "# 1. Stopwords dan TF-IDF dari teks\n",
    "indonesian_stopwords = stopwords.words('indonesian')\n",
    "data['Combined Summary'] = data['Learning Path'] + ' ' + data['Learning Path Summary'] + ' ' + data['Course Name_x'] + ' ' + data['Course Summary']\n",
    "tfidf = TfidfVectorizer(stop_words=indonesian_stopwords)\n",
    "tfidf_matrix = tfidf.fit_transform(data['Combined Summary']).toarray()  # Ubah ke array agar bisa digunakan\n",
    "# 2. Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(tfidf_matrix)\n",
    "\n",
    "# 1. Memproses data numerik dengan Autoencoder\n",
    "def create_autoencoder_model(input_dim):\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    encoded = Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_layer)\n",
    "    encoded = LeakyReLU(negative_slope=0.01)(encoded)\n",
    "    encoded = Dropout(0.3)(encoded)\n",
    "    encoded = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(0.01))(encoded)\n",
    "    encoded = LeakyReLU(negative_slope=0.01)(encoded)\n",
    "    encoded = Dropout(0.3)(encoded)\n",
    "    encoded = Dense(32, kernel_regularizer=tf.keras.regularizers.l2(0.01))(encoded)\n",
    "    \n",
    "    decoded = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(0.01))(encoded)\n",
    "    decoded = LeakyReLU(negative_slope=0.01)(decoded)\n",
    "    decoded = Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01))(decoded)\n",
    "    decoded = LeakyReLU(negative_slope=0.01)(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    autoencoder = tf.keras.models.Model(input_layer, decoded)\n",
    "    return autoencoder\n",
    "\n",
    "# Latih Autoencoder dan dapatkan encoder\n",
    "def train_and_get_encoder(X_scaled):\n",
    "    input_dim = X_scaled.shape[1]\n",
    "    autoencoder = create_autoencoder_model(input_dim)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    autoencoder.compile(optimizer=optimizer, loss=Huber())\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    autoencoder.fit(\n",
    "        X_scaled, X_scaled,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Ambil bagian encoder dari Autoencoder\n",
    "    encoder = tf.keras.models.Model(inputs=autoencoder.input, outputs=autoencoder.layers[-4].output)\n",
    "    return autoencoder, encoder\n",
    "\n",
    "    # Mengambil representasi dari layer encoded\n",
    "    encoder = tf.keras.models.Model(inputs=autoencoder.input, outputs=autoencoder.layers[-4].output)\n",
    "    return encoder.predict(X_scaled)\n",
    "\n",
    "# 2. Memproses data teks dengan TF-IDF\n",
    "def preprocess_learning_paths(data):\n",
    "    data['combined_features'] = data.apply(lambda x: f\"{x['Learning Path']} {x['Learning Path Summary']} {x['Course Name_x']}\", axis=1)\n",
    "    return data\n",
    "    \n",
    "def encode_with_autoencoder(data, autoencoder):\n",
    "    encoder = tf.keras.models.Model(inputs=autoencoder.input, outputs=autoencoder.layers[-4].output)  # Layer sebelum bagian decoding\n",
    "    return encoder.predict(data)\n",
    "\n",
    "def vectorize_learning_paths(data):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    return vectorizer.fit_transform(data['combined_features'])\n",
    "\n",
    "# 3. Menggabungkan hasil Autoencoder dan TF-IDF\n",
    "def combine_features(numeric_embeddings, tfidf_embeddings):\n",
    "    # Gabungkan numeric embeddings dan tf-idf embeddings menjadi satu fitur\n",
    "    combined = hstack([numeric_embeddings, tfidf_embeddings])  # Menyatu dengan cara yang benar\n",
    "    return combined\n",
    "\n",
    "# 4. Clustering dan rekomendasi\n",
    "def cluster_and_recommend(data, user_preferences):\n",
    "    # Proses data teks\n",
    "    data = preprocess_learning_paths(data)\n",
    "    \n",
    "    # Dapatkan embedding dari encoder yang dilatih\n",
    "    numeric_features = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Latih autoencoder dan ambil encoder\n",
    "    autoencoder, encoder = train_and_get_encoder(X_scaled)\n",
    "    \n",
    "    # Gunakan encoder untuk mendapatkan fitur numerik\n",
    "    numeric_embeddings = encoder.predict(numeric_features.values)\n",
    "    \n",
    "    # Ekstraksi fitur teks dengan TF-IDF\n",
    "    tfidf_vectors = vectorize_learning_paths(data)\n",
    "    \n",
    "    # Gabungkan fitur numerik dan teks\n",
    "    combined_features = combine_features(numeric_embeddings, tfidf_vectors)\n",
    "    \n",
    "    # Clustering\n",
    "    num_clusters = 5\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "    clusters = kmeans.fit_predict(combined_features)\n",
    "    data['cluster'] = clusters\n",
    "    \n",
    "    # Rekomendasi\n",
    "    user_vector = \" \".join(user_preferences)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    all_vectors = vectorizer.fit_transform(data['combined_features'].tolist() + [user_vector])\n",
    "    user_vector_tfidf = all_vectors[-1]\n",
    "    \n",
    "    # Gabungkan fitur numerik dan TF-IDF untuk user_vector\n",
    "    user_combined_features = combine_features(numeric_embeddings, all_vectors[:-1])\n",
    "    \n",
    "    similarity_scores = cosine_similarity(user_vector_tfidf, user_combined_features)\n",
    "    data['similarity'] = similarity_scores.flatten()\n",
    "    \n",
    "    recommendations = data.sort_values(by='similarity', ascending=False).head(3)\n",
    "    \n",
    "    return recommendations, combined_features, clusters\n",
    "\n",
    "# 5. Visualisasi kluster\n",
    "def visualize_clusters(combined_features, clusters):\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(combined_features)\n",
    "    \n",
    "    # PCA results\n",
    "    pca_df = pd.DataFrame(reduced_data, columns=['PCA1', 'PCA2'])\n",
    "    pca_df['cluster'] = clusters\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(data=pca_df, x='PCA1', y='PCA2', hue='cluster', palette='Set1', alpha=0.7)\n",
    "    plt.title('Clustering of Learning Paths with Combined Features')\n",
    "    plt.show()\n",
    "\n",
    "# Contoh penggunaan\n",
    "user_preferences = [\"Machine Learning\", \"Python\", \"Data Science\"]  # Preferensi pengguna\n",
    "recommendations, combined_features, clusters = cluster_and_recommend(data, user_preferences)\n",
    "\n",
    "print(\"\\nTop 3 Recommended Learning Paths:\")\n",
    "print(recommendations[['Course Name_x', 'similarity']])\n",
    "\n",
    "# Visualisasi kluster\n",
    "visualize_clusters(combined_features, clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcae11f-3ed4-4dca-9ddc-bf1f7cb0dfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1edb0cb-f7be-42f5-bc1a-7c327ff031d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
